{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5848caff-1a4b-4781-ae1a-7096a663ec29",
   "metadata": {},
   "source": [
    "**Q1. What is Gradient Boosting Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266de5c8-1592-4bcd-8397-5cc5f9a71d2b",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "Gradient Boosting Regression (GBR) is a machine learning technique used for regression tasks, where the goal is to predict a continuous numerical value rather than a class label. It belongs to the family of boosting algorithms and is an extension of Gradient Boosting Machines (GBM), originally designed for classification tasks.\n",
    "\n",
    "### How Gradient Boosting Regression Works:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the model with a simple regression model, usually a decision tree with a small depth (often called a decision stump).\n",
    "\n",
    "2. **Iterative Training**:\n",
    "   - Sequentially train new regression trees (weak learners) to correct the errors made by the existing ensemble of trees.\n",
    "   - Each new tree is trained on the residuals (the difference between actual and predicted values) of the previous ensemble.\n",
    "\n",
    "3. **Gradient Descent Optimization**:\n",
    "   - Gradient Boosting Regression optimizes the model by minimizing a loss function, typically using gradient descent methods.\n",
    "   - The loss function measures the difference between predicted values and actual target values.\n",
    "\n",
    "4. **Gradient Calculation**:\n",
    "   - Compute the gradient (partial derivative) of the loss function with respect to the predicted values.\n",
    "   - Use this gradient to update the predictions in the direction that minimizes the loss.\n",
    "\n",
    "5. **Additive Model Building**:\n",
    "   - The final prediction is made by summing the predictions of all regression trees, weighted by a learning rate.\n",
    "   - Each tree is built to correct the residuals left by the previous trees, making the model more accurate with each iteration.\n",
    "\n",
    "### Advantages of Gradient Boosting Regression:\n",
    "\n",
    "- **High Predictive Accuracy**: Gradient Boosting Regression often yields highly accurate predictions due to its ability to capture complex relationships in the data.\n",
    "  \n",
    "- **Handles Non-linearity**: It can model non-linear relationships between features and target variables effectively.\n",
    "\n",
    "- **Robust to Overfitting**: Through regularization techniques like learning rate adjustment and tree pruning, Gradient Boosting Regression can mitigate overfitting.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Predictive Modeling**: Used extensively in various fields such as finance, healthcare, and marketing for predicting continuous outcomes.\n",
    "  \n",
    "- **Feature Importance**: Provides insights into which features are most influential in predicting the target variable.\n",
    "\n",
    "- **Time Series Forecasting**: Suitable for forecasting future values based on historical data.\n",
    "\n",
    "Gradient Boosting Regression, implemented in libraries like scikit-learn (as `GradientBoostingRegressor`) and XGBoost, is a powerful tool for regression tasks where high accuracy and interpretability are crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba948aa7-6837-4c95-99b9-5ab19bb54a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80300d01-9947-41e3-b091-667f87a1ff09",
   "metadata": {},
   "source": [
    "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171baa06-5fdc-4b99-8cb5-45841af3dbd2",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "Implementing a gradient boosting algorithm from scratch using Python and NumPy involves several steps, including defining the base learner (weak learner), implementing the gradient descent optimization, and iterating through multiple boosting rounds. Hereâ€™s a simplified example for a regression problem:\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "- **GradientBoostingRegressor Class**: Implements the gradient boosting algorithm. It initializes with the mean of y and iteratively fits weak learners (DecisionStumps) to residuals, updating predictions with each iteration.\n",
    "  \n",
    "- **DecisionStump Class**: Represents a simple weak learner (a decision stump) that splits based on a single feature and value.\n",
    "\n",
    "- **Evaluation**: After training, the model is evaluated on a test set using Mean Squared Error (MSE) and R-squared metrics.\n",
    "\n",
    "This example provides a basic framework for understanding how gradient boosting works and how to implement it from scratch using Python and NumPy. For practical applications, consider optimizing further, handling more complex datasets, and incorporating additional features such as early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a25cc3-03db-4381-8404-00caa4ea1c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 664.20\n",
      "R-squared: 0.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Function to calculate mean squared error (MSE)\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Function to calculate R-squared\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of y\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(initial_prediction)\n",
    "        \n",
    "        # Iterate to train n_estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - self.predict(X)\n",
    "            \n",
    "            # Fit a weak learner (simple decision stump in this case)\n",
    "            weak_learner = DecisionStump()\n",
    "            weak_learner.fit(X, residuals)\n",
    "            \n",
    "            # Predict using the weak learner\n",
    "            prediction = weak_learner.predict(X)\n",
    "            \n",
    "            # Update the model (additive model)\n",
    "            self.models.append(weak_learner)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing predictions from all weak learners\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models[1:]:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions + self.models[0]  # Add initial prediction (mean of y)\n",
    "\n",
    "# Simple decision stump weak learner (for demonstration)\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Find the best split (for simplicity, use a single feature and value)\n",
    "        self.split_feature = 0  # Using only the first feature for simplicity\n",
    "        self.split_value = np.median(X[:, self.split_feature])\n",
    "        self.prediction = np.mean(y[X[:, self.split_feature] <= self.split_value])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.split_feature] <= self.split_value,\n",
    "                        self.prediction, -self.prediction)\n",
    "\n",
    "# Split data into train and test sets\n",
    "split = 80\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Initialize and train the gradient boosting regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e28bd-c2f3-4947-a737-772439c3164e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21454cf7-55be-4a0b-a2a9-34cb39226aeb",
   "metadata": {},
   "source": [
    "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ce856-0a97-4070-bf0a-99fefef41ca3",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "To optimize the performance of the Gradient Boosting Regression model, we can experiment with different hyperparameters such as learning rate, number of trees (estimators), and tree depth. Grid search and random search are common techniques to find the best combination of hyperparameters. Here's how you can perform hyperparameter optimization using GridSearchCV from scikit-learn:\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "- **Parameter Grid**: `param_grid` defines the grid of hyperparameters to search through, including `n_estimators` (number of trees), `learning_rate` (shrinkage), and `max_depth` (maximum depth of each tree).\n",
    "  \n",
    "- **GridSearchCV**: `GridSearchCV` is used to perform a cross-validated grid search over the parameter grid. It optimizes the model based on the negative mean squared error (`neg_mean_squared_error`) as the scoring metric.\n",
    "\n",
    "- **Fit and Evaluation**: The best parameters and the best model obtained from `GridSearchCV` are printed. Then, predictions are made on the test set using the best model, and performance metrics (MSE and R-squared) are evaluated.\n",
    "\n",
    "### Notes:\n",
    "- Adjust the `param_grid` and other settings based on your specific dataset and problem requirements.\n",
    "- Grid search is exhaustive and may be computationally expensive for large parameter grids. Randomized search (`RandomizedSearchCV`) is an alternative that samples a fixed number of parameter settings from the specified distributions.\n",
    "- Cross-validation (`cv=5` in this example) is used to ensure robustness of the model evaluation by splitting the data into multiple folds.\n",
    "\n",
    "This approach helps in finding the optimal hyperparameters for the Gradient Boosting Regression model, leading to improved predictive performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1aa84fb-ca59-4456-8650-9279d24eb362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best Negative MSE Score: -40.138785939422505\n",
      "\n",
      "Evaluation on Test Set:\n",
      "Mean Squared Error (MSE): 37.75\n",
      "R-squared: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Negative MSE Score:\", grid_search.best_score_)\n",
    "\n",
    "# Predictions on the test set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nEvaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdea6b-82a6-4a35-bddb-77e88675aae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efcd8c8c-c9a9-4c4c-9a8e-2382cc096ef3",
   "metadata": {},
   "source": [
    "**Q4. What is a weak learner in Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd0f3e-2997-411f-a798-94d69bcd6b68",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "In the context of Gradient Boosting, a weak learner refers to a simple predictive model that performs slightly better than random guessing on a classification or regression task. Specifically, in Gradient Boosting algorithms such as Gradient Boosting Machines (GBM) or AdaBoost, weak learners are typically decision trees with shallow depth or limited complexity. Here are key characteristics of a weak learner in Gradient Boosting:\n",
    "\n",
    "1. **Simple Model**: Weak learners are deliberately kept simple to ensure that they do not overfit the training data individually. For decision trees, this often means using trees with very few nodes or shallow depth (often referred to as decision stumps).\n",
    "\n",
    "2. **Performance Slightly Above Chance**: A weak learner is expected to perform better than random guessing but may still have a relatively high error rate compared to more complex models.\n",
    "\n",
    "3. **Training on Weighted Data**: In boosting algorithms like AdaBoost or Gradient Boosting, each weak learner is trained sequentially on a weighted version of the training data. These weights emphasize examples that were misclassified by previous weak learners, thereby focusing subsequent learners on more difficult cases.\n",
    "\n",
    "4. **Contribution to Ensemble**: Despite their simplicity and individual performance, weak learners contribute incrementally to the ensemble model's predictive power. Each weak learner corrects the errors (residuals) of the previous ensemble, leading to a collectively strong learner.\n",
    "\n",
    "5. **Example in Decision Trees**: In the context of decision trees, a weak learner might be a tree with only one split (decision stump), where the decision is based on a single feature and a threshold. This simplistic model helps in gradually improving predictions in boosting iterations.\n",
    "\n",
    "6. **Versatility**: While decision stumps are common as weak learners, the concept of weak learners can extend to other types of models like linear models, neural networks, or even more complex models in different contexts of boosting.\n",
    "\n",
    "In summary, a weak learner in Gradient Boosting is a modest predictive model that, when combined with other weak learners in a sequential manner, contributes to the overall robustness and accuracy of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5aa50-5403-4c54-9f17-e43374b8afe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5236164-1e9b-4d79-b071-e61a490fe62b",
   "metadata": {},
   "source": [
    "**Q5. What is the intuition behind the Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6afdf-d6c9-4b6b-8b82-5e5a5858f042",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm revolves around building an ensemble model of weak learners (typically decision trees) sequentially, where each learner corrects the errors made by its predecessors. Here's a more detailed intuition:\n",
    "\n",
    "1. **Sequential Improvement**: Gradient Boosting is an iterative algorithm that builds an ensemble of weak learners one at a time. Each new learner focuses on capturing the errors (residuals) of the previous ensemble, gradually reducing the overall error.\n",
    "\n",
    "2. **Gradient Descent Optimization**: The algorithm optimizes a loss function by iteratively minimizing the errors in predictions. It computes the gradient (partial derivative) of the loss function with respect to the predicted values, which guides the algorithm in the direction that minimizes the loss.\n",
    "\n",
    "3. **Additive Modeling**: Each weak learner is added to the ensemble in an additive manner. The predictions of all weak learners are combined, weighted by a small learning rate, to form the final prediction. This additive approach ensures that the ensemble model learns from the mistakes of each weak learner and improves iteratively.\n",
    "\n",
    "4. **Focus on Residuals**: Unlike other ensemble methods that focus on reducing variance, Gradient Boosting primarily focuses on reducing bias by fitting each new model to the residuals of the current ensemble. This process ensures that the model can capture complex patterns and relationships in the data that may have been missed initially.\n",
    "\n",
    "5. **Regularization and Learning Rate**: Gradient Boosting includes regularization techniques (like tree pruning) and a learning rate parameter to prevent overfitting and control the contribution of each weak learner to the ensemble. This ensures that the final model generalizes well to unseen data.\n",
    "\n",
    "6. **Versatility**: Gradient Boosting is versatile and can be applied to both regression and classification tasks. It can handle various types of data and is robust to noisy data, making it a popular choice in machine learning competitions and real-world applications.\n",
    "\n",
    "In essence, the intuition behind Gradient Boosting lies in the iterative improvement of predictions through the sequential addition of weak learners, guided by the gradient of a loss function. This approach results in a powerful ensemble model that combines the strengths of multiple simple models to achieve high predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20566ecc-ad50-44df-b9c9-f7749fe15032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649ddee4-936c-4cbb-babf-b89d230e2851",
   "metadata": {},
   "source": [
    "**Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd8df-abf9-4177-9748-7c69e59f6619",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners (typically decision trees) sequentially in a way that each learner corrects the errors made by its predecessors. Hereâ€™s a step-by-step explanation of how Gradient Boosting builds this ensemble:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with an initial prediction, often the mean (or median) of the target variable for regression or a constant value for classification.\n",
    "\n",
    "2. **Compute Initial Residuals**:\n",
    "   - Calculate the initial residuals as the difference between the actual target values and the initial prediction.\n",
    "\n",
    "3. **Iterative Training**:\n",
    "   - For each boosting round (iteration):\n",
    "     a. **Fit a Weak Learner**: Train a new weak learner (usually a decision tree with limited depth) on the current residuals. The weak learner is trained to predict the residuals left by the ensemble of learners built so far.\n",
    "     b. **Compute Learner Contribution**: Determine the contribution (weight) of the new weak learner to the ensemble. This is typically calculated using a learning rate (shrinkage) that scales the contribution of each tree.\n",
    "     c. **Update Ensemble Prediction**: Update the ensembleâ€™s prediction by adding the scaled prediction of the new weak learner to the previous ensemble prediction.\n",
    "     d. **Update Residuals**: Update the residuals by subtracting the predictions made by the new weak learner from the current residuals. This adjustment focuses subsequent learners on the errors (residuals) made by the ensemble so far.\n",
    "\n",
    "4. **Stop Criterion**: Repeat the process for a fixed number of iterations (controlled by `n_estimators` parameter) or until a predefined stopping criterion (e.g., no further improvement in the loss function) is met.\n",
    "\n",
    "5. **Final Ensemble Prediction**: The final prediction is the sum of predictions from all weak learners, scaled by their respective learning rates, and added to the initial prediction.\n",
    "\n",
    "### Key Points:\n",
    "- **Sequential Correction**: Each weak learner is trained to correct the errors (residuals) of the ensemble built so far, thereby gradually improving the predictive power of the ensemble.\n",
    "  \n",
    "- **Additive Modeling**: The ensemble prediction is formed additively, where each weak learner contributes incrementally to the final prediction, adjusted by a learning rate to control the update size.\n",
    "  \n",
    "- **Regularization**: Techniques like learning rate adjustment, tree pruning, and early stopping are employed to prevent overfitting and improve generalization performance.\n",
    "  \n",
    "- **Versatility**: Gradient Boosting can handle both regression and classification tasks and is effective with diverse types of data, making it widely used in various machine learning applications.\n",
    "\n",
    "By iteratively adding weak learners and adjusting the ensemble prediction based on their contributions, Gradient Boosting constructs a robust model that can capture complex relationships in the data and achieve high predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8ed2f-0e40-43bb-992b-e850f29976d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd518f13-fad4-40d1-b31d-dd70cabb2c8f",
   "metadata": {},
   "source": [
    "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976c2aa-1dfe-4a04-ad59-c44e136ac6e6",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "\n",
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the underlying principles of how weak learners are sequentially added to minimize a loss function. Here are the key steps involved in developing this intuition:\n",
    "\n",
    "1. **Loss Function**:\n",
    "   - Define a loss function \\( L(y, F(x)) \\) that measures the difference between the true target \\( y \\) and the predicted values \\( F(x) \\) of the ensemble model.\n",
    "\n",
    "2. **Initial Prediction**:\n",
    "   - Start with an initial prediction \\( F_0(x) \\), often the mean (or median) of the target variable \\( y \\).\n",
    "\n",
    "3. **Residual Calculation**:\n",
    "   - Compute the initial residuals \\( r_i = y_i - F_0(x_i) \\), where \\( y_i \\) are the true values and \\( x_i \\) are the input features.\n",
    "\n",
    "4. **Sequential Learning**:\n",
    "   - For each boosting round \\( m = 1, 2, \\ldots, M \\):\n",
    "     a. **Fit a Weak Learner**: Train a weak learner \\( h_m(x) \\) to predict the residuals \\( r_i \\). Typically, weak learners are shallow decision trees.\n",
    "     b. **Compute Learner Contribution**: Determine the contribution \\( \\gamma_m \\) of the weak learner \\( h_m(x) \\) by minimizing the loss function:\n",
    "        \\[ \\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{N} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)) \\]\n",
    "        Here, \\( F_{m-1}(x_i) \\) is the ensemble prediction up to iteration \\( m-1 \\).\n",
    "     c. **Update Ensemble**: Update the ensemble prediction:\n",
    "        \\[ F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x) \\]\n",
    "     d. **Update Residuals**: Update the residuals based on the new predictions:\n",
    "        \\[ r_i^{(m)} = r_i^{(m-1)} - \\gamma_m h_m(x_i) \\]\n",
    "\n",
    "5. **Final Prediction**:\n",
    "   - The final prediction \\( F_M(x) \\) is the sum of predictions from all weak learners:\n",
    "     \\[ F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\gamma_m h_m(x) \\]\n",
    "\n",
    "6. **Regularization**:\n",
    "   - Introduce regularization techniques such as learning rate \\( \\eta \\) to control the contribution of each weak learner:\n",
    "     \\[ F_m(x) = F_{m-1}(x) + \\eta \\gamma_m h_m(x) \\]\n",
    "\n",
    "7. **Stopping Criterion**:\n",
    "   - Stop the iterative process after a fixed number of boosting rounds \\( M \\) or when a certain criterion (e.g., no further improvement in the loss function) is met.\n",
    "\n",
    "### Mathematical Intuition:\n",
    "- **Gradient Descent**: Each weak learner (tree) is trained to minimize the gradient of the loss function with respect to the predictions of the ensemble up to that point. This ensures that each subsequent learner focuses on reducing the residuals (errors) left by the previous ensemble.\n",
    "\n",
    "- **Additive Modeling**: The ensemble prediction is built additively, with each weak learner adjusting the predictions to minimize the overall loss function. This iterative approach leads to an ensemble model that is capable of capturing complex relationships in the data.\n",
    "\n",
    "- **Bias-Variance Trade-off**: Gradient Boosting aims to reduce bias by fitting each new weak learner to the residuals, while regularization techniques like learning rate control help manage variance and prevent overfitting.\n",
    "\n",
    "By following these steps and understanding the iterative nature of how weak learners are sequentially added and optimized in Gradient Boosting, one can develop a clear mathematical intuition behind the algorithm's construction and functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93127bf-68c4-4dd0-bf09-45f0e6d9eedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a355e-5951-4710-b28c-ec74bc3774bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
